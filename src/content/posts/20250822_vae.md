---
title: Auto-Encoding Variational Bayes - notes
pubDate: 2025-08-22
categories: ["ML/AI"]
description: "Diederik P. Kingma, Max Welling. Auto-Encoding Variational Bayes. https://arxiv.org/abs/1312.6114, 2013."
slug: VAE
---

言わずと知れた VAE 論文だが, 理論的な骨子は次の２点.
1. 真の事後分布の近似として, 認識モデルを導入
2. 再パラメータ化を行い, 効率的な推定器を開発

## 認識モデル $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ の導入

*変分下限* (*variational lower bound*) $\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})$ の最大化に帰着.

-- 記法と仮定 --
- データセット $\mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^N$, $\mathrm{i.i.d.}$ な $N$ 個の連続サンプル $\mathbf{x}^{(i)}$ の集合.
- サンプル $\mathbf{x}^{(i)}$ は次のプロセスで無作為に生成されると仮定.
  1. $\mathbf{\theta}$ によりパラメータ付けされた事前分布 $p_{\mathbf{\theta}}(\mathbf{z})$ が連続潜在変数（*code*）$\mathbf{z}^{(i)}$ を生成.
  2. その下での条件付き分布（*decoder*）$p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})$ が $\mathbf{x}^{(i)}$ を生成.
- $p_{\mathbf{\theta}}(\mathbf{z})$ と $p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})$ は $\mathrm{a.e.}$ で $\mathbf{\theta}$ と $\mathbf{z}$ に関して偏微分可能と仮定.
- 認識モデル（*encoder*）$q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$, 真の事後分布 $p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})$ の近似.
- 周辺尤度 $\log p_{\mathbf{\theta}}(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})=\sum_{i=1}^N\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)})$.

-- $\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)})$ の計算 --
$$
\begin{align}
&\space\space\space\space\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}) \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)})\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[p_{\mathbf{\theta}}(\mathbf{x}^{(i)},\mathbf{z})\frac{p_{\mathbf{\theta}}(\mathbf{x}^{(i)})}{p_{\mathbf{\theta}}(\mathbf{x}^{(i)},\mathbf{z})}\right]\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[\frac{p_{\mathbf{\theta}}(\mathbf{x}^{(i)},\mathbf{z})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})}\right]\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[\frac{p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})p_{\mathbf{\theta}}(\mathbf{z})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})}\right]\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[\frac{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})}\frac{p_{\mathbf{\theta}}(\mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]\mathrm{d}\mathbf{z} \\
&= \mathit{D_{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})\right) \\
&\space\space-\mathit{D_{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\mathbf{\theta}}(\mathbf{z})\right)+\mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] \\
&\coloneqq\mathit{D_{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})\right)+\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})
\end{align}
$$

-- $\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})$ 各項の解釈 --
1. $\mathit{D_{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})||p_{\mathbf{\theta}}(\mathbf{z})\right)$: 正則化項, エンコーダが事前分布から離れ過ぎないように圧力をかける. 小さくしたい.
2. $\mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]$: 期待される負の再構成誤差, デコーダによる尤度が高ければ誤差は小さい. 大きくしたい.

## 再パラメータ化トリック

変分計算の秘訣として, 変分下限に新たな表現（推定器）を与える.

## Variational Auto-Encoder (VAE)

$p_{\mathbf{\theta}}(\mathbf{z})=\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I})$, $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})=\mathcal{N}(\mathbf{z};\mathbf{\mu}^{(i)},\mathbf{\sigma}^{2(i)}\mathbf{I})$ としたものを *VAE* と呼ぶ。
