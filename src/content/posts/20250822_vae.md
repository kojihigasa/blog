---
title: Auto-Encoding Variational Bayes - notes
pubDate: 2025-08-22
categories: ["ML/AI"]
description: "Diederik P. Kingma, Max Welling. Auto-Encoding Variational Bayes. https://arxiv.org/abs/1312.6114, 2013."
slug: VAE
---

言わずと知れた VAE 論文だが, 理論的な骨子は次の２点.

1. 真の事後分布の近似として, 認識モデルを導入
2. 再パラメータ化を行い, 効率的な推定器を開発

## 認識モデル $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ の導入

*変分下限* (*variational lower bound*) $\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})$ の最大化に帰着.

-- 記法と仮定 --

- データセット $\mathbf{X}=\{\mathbf{x}^{(i)}\}_{i=1}^N$, $\mathrm{i.i.d.}$ な $N$ 個の連続サンプル $\mathbf{x}^{(i)}$ の集合.
- サンプル $\mathbf{x}^{(i)}$ は次のプロセスで無作為に生成されると仮定.
  1. $\mathbf{\theta}$ によりパラメータ付けされた事前分布 $p_{\mathbf{\theta}}(\mathbf{z})$ が連続潜在変数（*code*）$\mathbf{z}^{(i)}$ を生成.
  2. その下での条件付き分布（*decoder*）$p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})$ が $\mathbf{x}^{(i)}$ を生成.
- $p_{\mathbf{\theta}}(\mathbf{z})$ と $p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})$ は $\mathrm{a.e.}$ で $\mathbf{\theta}$ と $\mathbf{z}$ に関して偏微分可能と仮定.
- 認識モデル（*encoder*）$q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$, 真の事後分布 $p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x})$ の近似.
- 周辺尤度 $\log p_{\mathbf{\theta}}(\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(N)})=\sum_{i=1}^N\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)})$.

-- $\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)})$ の計算 --

$$
\begin{align}
&\space\space\space\space\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}) \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)})\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[p_{\mathbf{\theta}}(\mathbf{x}^{(i)},\mathbf{z})\frac{p_{\mathbf{\theta}}(\mathbf{x}^{(i)})}{p_{\mathbf{\theta}}(\mathbf{x}^{(i)},\mathbf{z})}\right]\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[\frac{p_{\mathbf{\theta}}(\mathbf{x}^{(i)},\mathbf{z})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})}\right]\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[\frac{p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})p_{\mathbf{\theta}}(\mathbf{z})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})}\right]\mathrm{d}\mathbf{z} \\
&= ∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log\left[\frac{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}{p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})}\frac{p_{\mathbf{\theta}}(\mathbf{z})}{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]\mathrm{d}\mathbf{z} \\
&= D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})\right) \\
&\space\space-D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z})\right)+\mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right] \\
&≔ D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z}|\mathbf{x}^{(i)})\right)+\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})
\end{align}
$$

-- $\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})$ 各項の解釈 --

1. $D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z})\right)$: 正則化項, エンコーダが事前分布から離れ過ぎないように圧力をかける. 小さくしたい.
2. $\mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z})\right]$: 期待される負の再構成誤差, デコーダによる尤度が高ければ誤差は小さい. 大きくしたい.

## 再パラメータ化トリック

ランダム変数を決定論的変数へ書き換えることで, 変分下限に新たな表現（推定器）を与える.

-- トリック --

ランダムサンプリング $\mathbf{z}∼q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x})$ を連続変数, かつパラメータ $\mathbf{\phi}$ に対して微分可能と仮定すると, ある独立周辺分布 $p(\mathbf{ϵ})$ によってサンプルされるノイズ変数 $\mathbf{ϵ}$, および $\mathbf{\phi}$ によってパラメータ付けされたあるベクトル値関数 $g_{\mathbf{\phi}}(.)$ を用いて, 潜在変数 $\mathbf{z}$ は次のように再パラメータ化される.
$$
\mathbf{z} = g_{\mathbf{\phi}}(\mathbf{ϵ},\mathbf{x})\space\space\mathrm{with}\space\space \mathbf{{ϵ}}∼p(\mathbf{{ϵ}})
$$
$p(\mathbf{ϵ})$ や $g_{\mathbf{\phi}}(.)$ は以下のように選べばいい.
![reparameterization](/blog/20250822_vae_reparameterization.png)

-- 確率勾配変分 Bayes (SGVB) 推定器 $\tilde{\mathcal{L}}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})$ --

$$
\begin{align}
&\space\space\space\space\tilde{\mathcal{L}}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)}) \\
&= -D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z})\right)+\mathbb{E}_{q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})}\left[\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|g_{\mathbf{\phi}}(\mathbf{ϵ},\mathbf{x}^{(i)}))\right] \\
&= -D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z})\right)+∫q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|g_{\mathbf{\phi}}(\mathbf{ϵ},\mathbf{x}^{(i)}))\mathrm{d}\mathbf{z} \\
&= -D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z})\right)+∫p(\mathbf{ϵ})\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|g_{\mathbf{\phi}}(\mathbf{ϵ},\mathbf{x}^{(i)}))\mathrm{d}\mathbf{ϵ} \\
&≃ -D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z})\right)+\frac{1}{L}\sum_{l=1}^L\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|g_{\mathbf{\phi}}(\mathbf{ϵ}^{(l)},\mathbf{x}^{(i)})) \\
&\space\space∵ \mathrm{Monte\ Carlo\ 法} \\
&≔ -D_{\text{KL}}\left(q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})\|p_{\mathbf{\theta}}(\mathbf{z})\right)+\frac{1}{L}\sum_{l=1}^L\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})
\end{align}
$$

-- Auto-Encoding 変分 Bayes (AEVB) アルゴリズム --

最終的なアルゴリズム. データの一部 ($N$ データ点の内 $M$ 点) により結果を再現する, 次のミニバッチ法も含む.
$$
\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})≃\tilde{\mathcal{L}}^M(\mathbf{\theta},\mathbf{\phi};\mathbf{X}^{M})=\frac{N}{M}\sum_{i=1}^M\tilde{\mathcal{L}}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)})
$$
![AEVB](/blog/20250822_vae_aevb.png)

## 変分 Auto-Encoder (VAE)

以下のように選んだものを VAE と呼ぶ.

1. $p_{\mathbf{\theta}}(\mathbf{z})=\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I})$.
2. $p_{\mathbf{\theta}}(\mathbf{x}|\mathbf{z})=$多変量 Gauss/Bernoulli 分布. パラメータ $\mathbf{\theta}$ は多層パーセプトロン (MLP) により $\mathbf{z}$ から計算される.
3. $q_{\mathbf{\phi}}(\mathbf{z}|\mathbf{x}^{(i)})=\mathcal{N}(\mathbf{z};\mathbf{\mu}^{(i)},\mathbf{\sigma}^{2(i)}\mathbf{I})$. 平均 $\mathbf{\mu}$, および標準偏差 $\mathbf{\sigma}$ (併せてパラメータ $\mathbf{\phi}$) も MLP により $\mathbf{x}^{(i)}$ から計算される.

-- 推定器の具体的表式 --

$$
\begin{align}
&\space\space\space\space\mathcal{L}(\mathbf{\theta},\mathbf{\phi};\mathbf{x}^{(i)}) \\
&≃ -D_{\text{KL}}\left(\mathcal{N}(\mathbf{z};\mathbf{\mu}^{(i)},\mathbf{\sigma}^{2(i)}\mathbf{I})\|\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I})\right)+\frac{1}{L}\sum_{l=1}^L\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}) \\
&= \mathbb{E}_{\mathcal{N}(\mathbf{z};\mathbf{\mu}^{(i)},\mathbf{\sigma}^{2(i)}\mathbf{I})}\left[\log\mathcal{N}(\mathbf{z};\mathbf{0},\mathbf{I})-\log\mathcal{N}(\mathbf{z};\mathbf{\mu}^{(i)},\mathbf{\sigma}^{2(i)}\mathbf{I})\right] \\
&\space\space+\frac{1}{L}\sum_{l=1}^L\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}) \\
&= \mathbb{E}_{\mathcal{N}(\mathbf{z};\mathbf{\mu}^{(i)},\mathbf{\sigma}^{2(i)}\mathbf{I})}\left[-\frac{1}{2}\sum_{j=1}^J\left\lbrace\log(2\pi)+z_j^{2(i)}\right\rbrace+\frac{1}{2}\sum_{j=1}^J\left\lbrace\log(2\pi\sigma_j^{2(i)})+\frac{\left(z_j^{(i)}-\mu_j^{(i)}\right)^2}{\sigma_j^{2(i)}}\right\rbrace\right] \\
&\space\space+\frac{1}{L}\sum_{l=1}^L\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}) \\
&= \frac{1}{2}\sum_{j=1}^J\left\lbrace-\log(2\pi)-\left(\mu_j^{2(i)}+\sigma_j^{2(i)}\right)+\log(2\pi\sigma_j^{2(i)})+\frac{\sigma_j^{2(i)}}{\sigma_j^{2(i)}}\right\rbrace \\
&\space\space+\frac{1}{L}\sum_{l=1}^L\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)}) \\
&= \frac{1}{2}\sum_{j=1}^J\left(1+\log(\sigma_j^{2(i)})-\mu_j^{2(i)}-\sigma_j^{2(i)}\right)+\frac{1}{L}\sum_{l=1}^L\log p_{\mathbf{\theta}}(\mathbf{x}^{(i)}|\mathbf{z}^{(i,l)})
\end{align}
$$
ここで, Hadamard 積 $\odot$ として, $\mathbf{z}^{(i,l)}=\mathbf{\mu}^{(i)}+\mathbf{\sigma}^{(i)}\odot\mathbf{ϵ}^{(l)}\space\space\mathrm{and}\space\space\mathbf{ϵ}^{(l)}∼\mathcal{N}(\mathbf{0},\mathbf{I})$.
