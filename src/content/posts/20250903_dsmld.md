---
title: Generative Modeling by Estimating Gradients of the Data Distribution - notes
pubDate: 2025-09-03
categories: ["ML/AI"]
description: "Yang Song, Stefano Ermon. https://arxiv.org/abs/1907.05600, 2019."
slug: DSMLD
---

スコアを予測しそれを用いてサンプリングを行うスコアベース手法の提示. 構成要素は次の 2 つ.

1. デノイジングスコアマッチング (DSM) によるスコア推定
2. 焼きなまし Langevin 動力学を用いたサンプリング

各々, ナイーブなスコアマッチング, および ナイーブな Langevin MCMC が抱える以下の課題を克服する発想となっている.

1. 多様体仮説のもとでの課題
   1. データが低次元多様体に押し込められているとき, その周囲の高次元多様体での勾配は定義できないから, スコアも定義不能となる.
   2. データ分布の台が高次元多様体にまで広がっていなければ, 目的関数によるスコア推定は一貫性を失う.
2. データの低密度領域が存在することによる課題
   1. 目的関数により推定されたスコアが正確性を失う.
   2. 低密度領域によって隔てられた 2 つのモードが存在するとき, それらの相対的な重みを再現する分布に収束しにくい.

## 記法

- データセット $\left\lbrace \mathbf{x}_i∈\mathbb{R}^D\right\rbrace_{i=1}^N$: $\mathrm{i.i.d.}$ なサンプル $\mathbf{x}∼p_{\mathrm{data}}(\mathbf{x})$ の集合.
- $p_{\mathrm{data}}(\mathbf{x})$ のスコア $\nabla_{\mathbf{x}}\log p_{\mathrm{data}}(\mathbf{x})$: $p_{\mathrm{data}}(\mathbf{x})$ の対数尤度に関する勾配.
- スコアネットワーク $\mathbf{s}_{\mathbf{θ}}:\mathbb{R}^D→\mathbb{R}^D$: スコアを予測するために訓練される $\mathbf{θ}$ によってパラメータ付けされた NN.

## スコアマッチングによるスコア推定

最小化されるべき目的関数を素直に書き下せば以下のようになる.

$$
\mathrm{MSE}(\mathbf{θ}) ≔ \frac{1}{2}\mathbb{E}_{p_{\mathrm{data}}(\mathbf{x})}\left[\left\|\mathbf{s}_{\mathbf{θ}}(\mathbf{x})-\nabla_{\mathbf{x}}\log p_{\mathrm{data}}(\mathbf{x})\right\|^2\right]
$$

しかしながらここにおいて $p_{\mathrm{data}}(\mathbf{x})$ が未知ゆえそのスコアは計算不能である.

-- 目的関数の計算可能な形への書き換え --

$\mathrm{MSE}(\mathbf{θ})$ を積分の形であらわに書く.
$$
∫_{\mathbf{x}∈\mathbb{R}^D}p_{\mathrm{data}}(\mathbf{x})\left[\frac{1}{2}\left\|\nabla_{\mathbf{x}}\log p_{\mathrm{data}}(\mathbf{x})\right\|^2-\nabla_{\mathbf{x}}\log p_{\mathrm{data}}(\mathbf{x})^{\mathrm{T}}⋅\mathbf{s}_{\mathbf{θ}}(\mathbf{x})+\frac{1}{2}\left\|\mathbf{s}_{\mathbf{θ}}(\mathbf{x})\right\|^2\right]\mathrm{d}\mathbf{x}
$$
第 1 項は $\mathbf{θ}$ に依存しない定数. 第 2 項は次のように書き直せる.
$$
\begin{align}
&\space\space\space\space-∫_{\mathbf{x}∈\mathbb{R}^D}p_{\mathrm{data}}(\mathbf{x})\nabla_{\mathbf{x}}\log p_{\mathrm{data}}(\mathbf{x})^{\mathrm{T}}⋅\mathbf{s}_{\mathbf{θ}}(\mathbf{x})\mathrm{d}\mathbf{x} \\
&= -\sum_i∫_{\mathbf{x}∈\mathbb{R}^D}p_{\mathrm{data}}(\mathbf{x})\frac{∂\log p_{\mathrm{data}}(\mathbf{x})}{∂x_i}\mathbf{s}_{\mathbf{θ}}(\mathbf{x})_i\mathrm{d}\mathbf{x} \\
&= -\sum_i∫_{\mathbf{x}∈\mathbb{R}^D}\frac{∂p_{\mathrm{data}}(\mathbf{x})}{∂x_i}\mathbf{s}_{\mathbf{θ}}(\mathbf{x})_i\mathrm{d}\mathbf{x} \\
&= \sum_i∫_{\mathbf{x}∈\mathbb{R}^D}p_{\mathrm{data}}(\mathbf{x})\frac{∂\mathbf{s}_{\mathbf{θ}}(\mathbf{x})_i}{∂x_i}\mathrm{d}\mathbf{x}\space\space∵\text{部分積分} \\
&= ∫_{\mathbf{x}∈\mathbb{R}^D}p_{\mathrm{data}}(\mathbf{x})\mathrm{tr}\left(\nabla_{\mathbf{x}}\mathbf{s}_{\mathbf{θ}}(\mathbf{x})\right)\mathrm{d}\mathbf{x}
\end{align}
$$
したがって目的関数を未知のスコアを用いない形式に書き表せた. ただし, 定数部分は省略した.
$$
\mathrm{MSE}(\mathbf{θ}) = \mathbb{E}_{p_{\mathrm{data}}(\mathbf{x})}\left[\mathrm{tr}\left(\nabla_{\mathbf{x}}\mathbf{s}_{\mathbf{θ}}(\mathbf{x})\right)+\frac{1}{2}\left\|\mathbf{s}_{\mathbf{θ}}(\mathbf{x})\right\|^2\right]
$$

しかしながら, $\mathbb{E}_{p_{\mathrm{data}}(\mathbf{x})}\left[\mathrm{tr}\left(\nabla_{\mathbf{x}}\mathbf{s}_{\mathbf{θ}}(\mathbf{x})\right)\right]$ の計算量は膨大である.

-- デノイジングスコアマッチング (DSM) --

そこで, 事前に決められたノイズ分布 $q_{σ}\left(\tilde{\mathbf{x}}|\mathbf{x}\right)$ により $p_{\mathrm{data}}(\mathbf{x})$ に摂動を加えることで, 目的関数を素直な形のまま計算可能にし, 高価な Jacobian のトレース計算を回避する.

- 摂動後データ分布: $q_{σ}\left(\tilde{\mathbf{x}}\right) ≔ ∫q_{σ}\left(\tilde{\mathbf{x}}|\mathbf{x}\right)p_{\mathrm{data}}(\mathbf{x})\mathrm{d}\mathbf{x}$
- 目的関数: $\mathrm{MSE}(\mathbf{θ}) ≔ \frac{1}{2}\mathbb{E}_{q_{σ}\left(\tilde{\mathbf{x}}|\mathbf{x}\right)p_{\mathrm{data}}(\mathbf{x})}\left[\left\|\mathbf{s}_{\mathbf{θ}}(\tilde{\mathbf{x}})-\nabla_{\mathbf{x}}\log q_{σ}\left(\tilde{\mathbf{x}}|\mathbf{x}\right)\right\|^2\right]$

ここで $q_{σ}\left(\tilde{\mathbf{x}}|\mathbf{x}\right)$ のスコアが計算可能となることは具体例をもって明らかとなる. 例えば, 本論文で提示されているノイズ条件付きスコアネットワーク (NCSN) だと, $q_{σ}\left(\tilde{\mathbf{x}}\right)=\mathcal{N}\left(\tilde{\mathbf{x}}|\mathbf{x},σ^2\mathbf{I}\right)$ とし $\nabla_{\mathbf{x}}\log q_{σ}\left(\tilde{\mathbf{x}}|\mathbf{x}\right)=-\frac{1}{σ^2}\left(\tilde{\mathbf{x}}-\mathbf{x}\right)$.

## Langevin 動力学を用いたサンプリング

スコア関数のみを用いてサンプリングできるという著しい特徴を持つ. 固定ステップ長を $ϵ>0$, 事前分布 $π$ のもとで初期値を $\tilde{\mathbf{x}}∼π(\mathbf{x})$ とすれば, サンプリングは以下で与えられる.
$$
\tilde{\mathbf{x}}_t=\tilde{\mathbf{x}}_{t-1}+\frac{ϵ}{2}\nabla_{\mathbf{x}}\log p(\tilde{\mathbf{x}}_{t-1})+\sqrt{ϵ}\mathbf{z}_t\text{ where }\mathbf{z}_t∼\mathcal{N}(\mathbf{0},\mathbf{I})
$$
ただし, $\mathop{\lim}\limits_{ϵ→0,T→∞}\tilde{\mathbf{x}}_T=\mathbf{x}∼p_{\mathrm{data}}(\mathbf{x})$.

-- 焼きなまし Langevin 動力学を用いたサンプリング --

$\left\lbrace σ_i\right\rbrace_{i=1}^L$ を $\frac{σ_1}{σ_2}=\cdots=\frac{σ_{L-1}}{σ_L}>1$ を満たす正幾何級数として, 以下で与えられる.
![Langevin](/blog/20250903_dsmld_langevin.png)
